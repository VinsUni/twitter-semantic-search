\documentclass{acm_proc_article-sp}

\begin{document}

\title{Enabling Semantic Searching on Microblogs with Latent Semantic Analysis (Draft) 
\titlenote{This work is done with the help of Prof. Alfio Gliozzo and Dr. Or Biran in partial satisfaction of the requirements of E6998\_9 Spring 2013 course at Columbia University}
}

\numberofauthors{1} 

\author{\alignauthor Wei Wang, Yu Qiao, Qiuzi Shangguan, Ran Yu \\
\affaddr{Department of Computer Science} \\ 
\affaddr{Columbia University}
\affaddr{New York, USA} \\
\email{\{ww2315,yq2145,qs2130,ry2239\}@columbia.edu}
} 
\maketitle

\begin{abstract}
In our project, we implemented a LSA-based semantic search pipeline over tweets. More particularly, we used TREC 2011 microblog track corpus and its evaluation criteria. The results show that our algorithm yield a performance rise about 400\% compared with baseline using P@30. 
\end{abstract}

\section{Introduction}
Twitter is the most popular microblogging platform nowadays. There are more than 190 million active users on Twitter and they publish more than 65 million tweets every day \footnote{http://techcrunch.com/2010/06/08/twitter-190-million-users/}. Some research papers show that information retrieved from Twitter can be valuable when used in areas such as earthquake warning [1], opinion mining [2] and fresh website discovering [3]. Growing constantly and involving people's daily social activity, Twitter becomes a goldmine for big data and semantic research [4].\\
TREC (Text REtrieval Conference) encourages research in information retrieval from large text collections. The TREC workshop Microblog Track examines search tasks and evaluation methodologies for information seeking behaviors in microblogging environments \footnote{http://trec.nist.gov/pubs/call2012.html}. The 2012 Microblog Track focuses on addressing two search tasks: Real-time Adhoc and Real-time Filtering whereby a user's information need will be represented by a query at a specific time. In our project, we focus on Real-time Adhoc task: in the real-time search task, the user wishes to see the most recent but relevant information to the query. Hence, the system should answer a query by providing a list of relevant tweets ordered from newest to oldest, starting from the time the query was issued. When scoring tweets, systems should favor relevant and highly informative tweets about the query topic. \footnote{https://sites.google.com/site/microblogtrack/2012-guidelines}

\section{Corpus}
We use the official TREC\footnote{http://trec.nist.gov/} 2011 microblog twitter collection, aka Tweets2011, as our corpus\footnote{http://trec.nist.gov/data/tweets/}. The corpus contains approximately 16 million tweets sampled between January 23rd and February 8th, 2011. Although TREC committee provided us with a corpus dumper\footnote{https://github.com/lintool/twitter-tools}, for some reason it is broken now and we had to build such a tool by ourselves. We simply downloaded all the HTML pages containing such tweets and parse the tweet to get plain text. It really takes some time to dump such a large corpus. For us, the entire process lasted about 4 days using 5 different machines concurrently. For each tweet, we maintain the following information:
\begin{table}
\centering
\caption{Information Stored in Corpus}
\begin{tabular}{|l|l|l} \hline
TweetID&32957874548375552\\ \hline
Author&BlakeRamsey\\ \hline
Time&4:24 PM - 2 Feb 11\\ \hline
HashTag& \#Egypt\\ \hline
URL&http://bbc.in/hTfSea\\ \hline
Location&Union Market, Brooklyn\\ \hline
Tweet&Campaign fights 'major bus cuts\\ \hline
\hline\end{tabular}
\end{table}
Due to several reasons we finally get 11153327 tweets, instead of 16 million. Some reason might be: the deletion of a tweet, the protection of tweet, and network failure.

\section{Evaluation Criteria and Baseline}
As in \cite{soboroff2012evaluating}, TREC committee generated 49 topics that are covered by the corpus, and manually labeled some tweets and use it as evaluation criteria. More specifically, a group of researchers pooled results from multiple diverse retrieval systems, and then manually mark them as relevant, highly relevant, or non relevant. Then they use a statistical method \cite{voorhees2005trec} to generate a high quality test collection. The committee use a Perl script to evaluate each team's run. All the results are measured by P@30.
How we generated baseline (vs official baseline)\\

\section{Preprocessing}
Remove redundant characters(giiirrl)\\
Remove numbers\\
Remove non-english tweets\\
Dump URLs\\

\section{Rank model}
BM25\\
URL-enhanced ranking formula

\section{LSA}
Employing Mahout \footnote{http://mahout.apache.org/} and DefaultAnalyzer document vectorizer from Lucene, we first represent the corpus as a set of tf-df vectors [LSA 1], which is a occurrence matrix whose rows correspond to terms and whose columns corresponds to documents. Then we use LSA [LSA 2] to find a low rank approximation [LSA 3] to the occurrence matrix. A low rank approximation can be seen as a de-noisified matrix which helps to improve scoring later. Moreover, with a low rank approximation, we can finish essential computation within reasonable time limit.

To perform LSA and get a low rank approximation in an efficient way, we use distributed Stochastic SVD [LSA 4] tools from Mahout to decompose the initial occurrence matrix with a cluster. We also generate a factor matrix for Stochastic SVD on query vectors later. With a rank reduced occurrence matrix and a query vector projected to same space (rank reduced in the same way), we can easily score each document against the given query with cosine similarity [LSA 5].

\section{Evaluation Results}
results\\

\section{Conclusion}
What has been done 

\section{Future work}
What needs to be done

\section{Acknowledgments}
We would like to thank Prof. Alfio Massimiliano Gliozzo for his great class and illuminating discussion with us. Many thanks also go to Dr. Or Biran for his very patient instructions on our projects and detailed explanation. This work is impossible without the help of them. 

\bibliographystyle{abbrv}
\bibliography{waton13}
\balancecolumns
\end{document}
